# 集群状态

- green 健康状态，指所有主副分片都正常分配
- yellow 指所有主分片都正常分配，但是有副本分片未正常分配
- red 有主分片未分配

三种状态只是代表分片的工作状态，并不是代表整个es集群是否能够对外提供服务

# 故障转移

1. 集群由三个节点组成。node1所在机器宕机导致服务终止。node2和node3发现node1无法响应一段时间后会发起maser选举。此时由于主分片P0下线，集群状态变为red
2. master发现主分片P0未分配，将R0提升为主分片，集群状态变为yello

# 脑裂问题。

node2与node3会重新选举master，比如node2成为了新master，此时会更新cluster state。node1自己组成集群后，也会更新cluster state。同一个集群有两个master，而维护不同的cluster state，网络恢复后无法选择正确的maste。

解决方案为仅在可选举master-eligible节点数据大于等于quorum时才可以进行master选举。quorum = master-eligible 节点数/2 + 1。

设置config/elasticsearch.yml参数配置 discovery.zen.mininum_master_nodes为quorum即可避免脑裂。

# 分布式存储

## 存储算法

通过如下公式计算

shard = hash(routing)%number_of_primary_shards，

routing默认是文档id，number_of_primary_shards是主分片数

这也是分片数一旦确定后不能更改的原因。

## 原理

倒排索引不能更改，如果重新再替换实时性受到影响。所以新文档直接生成新的倒排索引文件，查询的时候同时查询所有的倒排文件，然后对查询结果做汇总计算即可。

Lucene构建的单个倒排索引称为segment，合在一起称为Index(Lucene中的名称)，ES中的一个Shard对应一个Lucene Index。Lucene会有一个专门的文件来记录所有的segment信息，称为 Commit Point

segment写入磁盘的过程依然很耗时，先将segment在缓存中创建并开放查询来进一步提升实时性，该过程在es中被称为refresh。在refresh之前文档会先存储在一个buffer中，refresh时将buffer中的所有文档清空并生成segmentes。默认每1秒执行一次refresh，因此文档的实时性被提高到1秒，这也是es被称为近实时（Near Real Time）的真正原因

如果在内存中的segment还没有写入磁盘前发生了宕机，那么内存中的文档就无法恢复了。es引入translog机制。写入文档到buffer时，同时将该操作写入translog。translog 文件会即时写入磁盘（fsync），6.x默认每个请求都会落盘，可以修改为每5秒写一次，这样风险便是丢失5秒内的数据，相关配置为index.translog.*es重新启动时会自动检查translog文件，并从中恢复数据。　

lush负责将内存中的segment写入磁盘，主要做如下的工作：将translog写入磁盘。将index buffer清空，其中的文档生成一个新的segment，相当于一个refrsh操作。更新commit point并写入磁盘。执行fsync操作，将内存中的segment写入磁盘。删除旧的translog文件　

refresh发生的时机主要有以下几种情况：间隔时间达到时，通过index.settings.refresh_interval来设定，默认是1秒。index.buffer占满时，其大小通过indices.memory.index_buffer_size设置，默认为jvm heap的10%，所有shard共享flush发生时也会发生refresh。间隔时间达到时，默认是30分钟。translog占满时。

segment一旦生成就不能更改，那么如果要删除文档，lucene会专门维护一个.del的文件，记录所有已经删除的文档，注意.del上记录的是文档在Lucene的内部id,在查询结果返回前会过滤掉.del中所有的文档

更新文档，首先删除文档，然后再创建新的文档　　

随着segment的增多，由于一次查询的segment数增多，查询速度会变慢,es会定时在后台进行segment merge的操作，减少segment的 数量，通过force_merge api可以手动强制做segment merge的操作。

# 集群选举

## 发起

master选举当然是由master-eligible节点发起，当一个master-eligible节点发现满足以下条件时发起选举：

1. 该master-eligible节点的当前状态不是master。
2. 该master-eligible节点通过ZenDiscovery模块的ping操作询问其已知的集群其他节点，没有任何节点连接到master。
3. 包括本节点在内，当前已有超过minimum_master_nodes个节点没有连接到master。

总结一句话，即当一个节点发现包括自己在内的多数派的master-eligible节点认为集群没有master时，就可以发起master选举。

## 选举规则

先根据节点的clusterStateVersion比较，clusterStateVersion越大，优先级越高。clusterStateVersion相同时，进入compareNodes，其内部按照节点的Id比较(Id为节点第一次启动时随机生成)。节点的Id越小，优先级越高。

## 过程

筛选activeMasters列表：的master就是从activeMasters列表或者masterCandidates列表选举出来，所以选举之前es首先需要得到这两个列表。Elasticsearch节点成员首先向集群中的所有成员发送Ping请求，elasticsearch默认等待discovery.zen.ping_timeout时间，然后elasticsearch针对获取的全部response进行过滤，筛选出其中activeMasters列表，activeMaster列表是其它节点认为的当前集群的Master节点

筛选masterCandidates列表：masterCandidates列表是当前集群有资格成为Master的节点。

从activeMasters列表选举Master节点：activeMaster列表是其它节点认为的当前集群的Master节点列表，如果activeMasters列表不为空，elasticsearch会优先从activeMasters列表中选举，也就是对应着流程图中的蓝色框，选举的算法是Bully算法

从masterCandidates列表选举Master节点：如果activeMaster列表为空，那么会在masterCandidates中选举，masterCandidates选举也会涉及到优先级比较

经过上述选举之后，会选举出一个准master节点。准master节点会等待其它节点的投票。

# 分布式搜索

ES的搜索会分两阶段进行，query和fetch。

query阶段。用户发出搜索请求到ES，节点收到请求后，会随机选择主副分片，发送查询请求。被选中的分片执行查询，然后排序。每个分片返回From+Size个排序后的文档id和排序值给节点。

fetch阶段。将query阶段从每个分片获取到的排序后文档id列表，重新排序，选取From到From+Size过文档的id。以mutli get请求的方式，到相应的分片获取详细的文档数据

每个分片都需要取form+size个文档，协调节点需要处理shard*(from+size)。每个分片都基于自己的分片上的数据进行相关性算法，导致打分偏移。

数据量不大时将主分片设置为1。数据量大时保证文档均匀分散在各个分片上。使用DFS Query Then Fetch，到每个分片把词频和文档频率进行搜集，进行完成的一次相关性算分。

# 联合查询

给定查询过滤条件 age=18 的过程就是先从 term index 找到 18 在 term dictionary 的大概位置，然后再从 term dictionary 里精确地找到 18 这个 term，然后得到一个 posting list 或者一个指向 posting list 位置的指针。然后再查询 gender= 女 的过程也是类似的。最后得出 age=18 AND gender= 女 就是把两个 posting list 做一个“与”的合并。

- 使用 skip list 数据结构。同时遍历 gender 和 age 的 posting list，互相 skip；
- 使用 bitset 数据结构，对 gender 和 age 两个 filter 分别求出 bitset，对两个 bitset 做 AN 操作。

Elasticsearch 支持以上两种的联合索引方式，如果查询的 filter 缓存到了内存中（以 bitset 的形式），那么合并就是两个 bitset 的 AND。如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历两个 on disk 的 posting list。